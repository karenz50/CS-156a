{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tZKpeI6Uodul"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def gradient(x):\n",
        "  u = x[0]\n",
        "  v = x[1]\n",
        "  grad_1 = 2 * (np.exp(v) + 2 * v * np.exp(-u)) * (u * np.exp(v) - 2 * v * np.exp(-u))\n",
        "  grad_2 = 2 * (u * np.exp(v) - 2 * np.exp(-u)) * (u * np.exp(v) - 2 * v * np.exp(-u))\n",
        "  return np.array([grad_1,grad_2])\n",
        "\n",
        "def error(x):\n",
        "  u = x[0]\n",
        "  v = x[1]\n",
        "  err = (u * np.exp(v) - 2 * v * np.exp(-u)) ** 2\n",
        "  return err\n",
        "\n",
        "w = np.array([1.0, 1.0])\n",
        "err = error(w)\n",
        "iter = 0\n",
        "max_iter = 20\n",
        "eta = .1\n",
        "\n",
        "while err > 10 ** -14 and iter < max_iter:\n",
        "    iter += 1\n",
        "\n",
        "    v_t = gradient(w) * eta\n",
        "\n",
        "    w = w - v_t\n",
        "    err = error(w)\n",
        "\n",
        "print(f'iterations = {iter}')\n",
        "print(f'final u, v = {w}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MsUcm3v-pAF1",
        "outputId": "55d235f4-f91d-444a-935a-e511c2f49ec9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "iterations = 10\n",
            "final u, v = [0.04473629 0.02395871]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "w = np.array([1.0, 1.0])\n",
        "err = error(w)\n",
        "\n",
        "for i in range(15):\n",
        "    v_t = gradient(w) * eta\n",
        "    w[0] -= v_t[0]\n",
        "    v_t = gradient(w) * eta\n",
        "    w[1] -= v_t[1]\n",
        "\n",
        "print(f'error = {error(w)}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rm8wGLKe53kl",
        "outputId": "91f20aca-7dcf-46d6-99e1-fb488fe175de"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "error = 0.13981379199615315\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def random_point():\n",
        "  return [np.random.uniform(-1, 1), np.random.uniform(-1, 1)]\n",
        "\n",
        "def find_y(x, m, b):\n",
        "  pos = m * x[0] - x[1] + b\n",
        "  return np.sign(pos)\n",
        "\n",
        "def generate_points(N):\n",
        "  pt1 = random_point()\n",
        "  pt2 = random_point()\n",
        "\n",
        "  m = (pt2[1] - pt1[1]) / (pt2[0] - pt1[0])\n",
        "  b = pt1[1] - m * pt1[0]\n",
        "\n",
        "  X = []\n",
        "  Y = []\n",
        "\n",
        "  for i in range(N):\n",
        "    new_pt = random_point()\n",
        "    y = find_y(new_pt, m, b)\n",
        "    X.append(new_pt)\n",
        "    Y.append(y)\n",
        "\n",
        "  ones = np.ones((N, 1))\n",
        "  X = np.hstack((ones, np.array(X)))\n",
        "\n",
        "  return np.array(X), np.array(Y)"
      ],
      "metadata": {
        "id": "319xl7rs6fjb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def gradient_logreg(X, Y, w, n):\n",
        "  return (-Y[n] * X[n]) / (1 + np.exp(Y[n] * np.dot(w.T, X[n])))\n",
        "\n",
        "def cross_entropy(X, Y, w, N):\n",
        "    sum = 0\n",
        "\n",
        "    for n in range(N):\n",
        "        sum += np.log(1 + np.exp(-Y[n] * np.dot(w.T, X[n])))\n",
        "\n",
        "    return sum / N"
      ],
      "metadata": {
        "id": "wYR6uwWrcVWd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "runs = 100\n",
        "N = 100\n",
        "test_N = 1000\n",
        "eta = .01\n",
        "stop = .01\n",
        "\n",
        "e_outs = []\n",
        "epochs = []\n",
        "\n",
        "for run in range(runs):\n",
        "    X, Y = generate_points(N + test_N)\n",
        "    train_X, train_Y = X[:N], Y[:N]\n",
        "    test_X, test_Y = X[N:], Y[N:]\n",
        "\n",
        "    w = np.zeros(3)\n",
        "    epoch = 0\n",
        "\n",
        "    while True:\n",
        "        old_w = w\n",
        "\n",
        "        shuffled = np.random.permutation(N)\n",
        "        train_X = train_X[shuffled]\n",
        "        train_Y = train_Y[shuffled]\n",
        "\n",
        "        for n in range(N):\n",
        "            w = w - eta * gradient_logreg(train_X, train_Y, w, n)\n",
        "\n",
        "        epoch += 1\n",
        "\n",
        "        if (np.linalg.norm(old_w - w) < stop):\n",
        "            break\n",
        "\n",
        "    e_out = cross_entropy(test_X, test_Y, w, test_N)\n",
        "    e_outs.append(e_out)\n",
        "    epochs.append(epoch)\n",
        "\n",
        "print(f\"Error {np.mean(e_outs)}\")\n",
        "print(f\"Epochs {np.mean(epochs)}\")"
      ],
      "metadata": {
        "id": "LRaFRzYlE6e0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "afcf09d1-7a26-4df9-8c60-5a9ed76e0f85"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error 0.1036175825012136\n",
            "Epochs 342.24\n"
          ]
        }
      ]
    }
  ]
}